{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0214210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import importlib\n",
    "import sys \n",
    "sys.path.append(\"/Users/zacharykelly/Documents/MATLAB/projects/lightLogger/raspberry_pi_firmware/utility\")\n",
    "import Pi_util\n",
    "import importlib\n",
    "import scipy.io \n",
    "from scipy.signal import find_peaks\n",
    "import matlab.engine\n",
    "import tqdm \n",
    "import warnings\n",
    "import virtual_foveation\n",
    "import hdf5storage\n",
    "import dill \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea53730",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'generator' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvirtual_foveation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_sensor_start_end_times\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/zacharykelly/Desktop/spatialFrequency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/applyVirtualFoveation/pythonCode/virtual_foveation.py:25\u001b[0m, in \u001b[0;36mfind_sensor_start_end_times\u001b[0;34m(path_to_recording)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Iterate over the sensors and fill in the start end time\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# if they existed\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sensor, files \u001b[38;5;129;01min\u001b[39;00m sensor_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Skip ununused snesors \u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m): \n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m \n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Load in the first and last files \u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'generator' has no len()"
     ]
    }
   ],
   "source": [
    "importlib.reload(virtual_foveation)\n",
    "virtual_foveation.find_sensor_start_end_times(\"/Users/zacharykelly/Desktop/spatialFrequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a playable video\n",
    "path_to_recording_chunks: str = \"/Volumes/T7 Shield/sam_gazecal_106\"\n",
    "output_dir: str = \"./\"\n",
    "Pi_util.generate_playable_videos(path_to_recording_chunks, output_dir, apply_digital_gain=True, fill_missing_frames=True, debayer_images=True, pupil_image_rotation_correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d049727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the frames of the recording \n",
    "world_frames: np.ndarray = Pi_util.destruct_video(\"./sam_gazecal_106/W.avi\", is_grayscale=True)\n",
    "print(world_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f1e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(world_frames[21000], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a88234",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_idx: np.ndarray = np.array([i for i in range(15000, 16000, 100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1894b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in world_frames[frame_idx]:\n",
    "    plt.imshow(frame, cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17529076",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat(\"sam106GazeCal_testframes.mat\", {\"frames\": world_frames[frame_idx]}) # 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc99b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate an array of transformed frames \n",
    "transformed_world_frames: np.ndarray = np.zeros_like(world_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4df5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the pupil gaze angles \n",
    "gaze_angles: np.ndarray = np.nan_to_num(scipy.io.loadmat(\"./gaze_angles\")[\"gaze_angles\"], 0)\n",
    "print(gaze_angles.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af3a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.io.savemat(\"sam106GazeCal_gaze_angles.mat\", {\"gaze_angles\": gaze_angles[frame_idx - 65]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27097863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the first chunk of both cameras. This will tell us if one or the other started first \n",
    "world_start_chunk_metadata: np.array = np.load(\"/Volumes/T7 Shield/sam_gazecal_106/world_time_2025DASH10DASH06_09COLON26COLON14DOT693525_chunk_0_metadata.npy\")\n",
    "pupil_start_chunk_metadata: np.array = np.load(\"/Volumes/T7 Shield/sam_gazecal_106/pupil_time_2025DASH10DASH06_09COLON26COLON15DOT252214_chunk_0_metadata.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the missing number of frames between the two measurements in time \n",
    "FPS: float = 120\n",
    "missing_frames: int = (abs(len(world_frames) - len(gaze_angles)))\n",
    "missing_time: float = missing_frames / FPS \n",
    "print(missing_frames)\n",
    "print(missing_time)\n",
    "\n",
    "# Print the start time of both cameras so we can see whcih is first \n",
    "print(world_start_chunk_metadata[0, 0] / (10 ** 9))\n",
    "print(pupil_start_chunk_metadata[0, 0])\n",
    "\n",
    "# Find the difference explained by this \n",
    "start_time_delay_time: float = abs(pupil_start_chunk_metadata[0, 0] - world_start_chunk_metadata[0, 0] / (10 ** 9))\n",
    "start_time_delay_frames: int = int(np.ceil(start_time_delay_time * FPS))\n",
    "print(start_time_delay_time)\n",
    "print(start_time_delay_frames)\n",
    "\n",
    "# Account for the fact that we measured the pupil camera time wise is 0.005 phase advanced \n",
    "pupil_phase_offset_seconds: float = 0.005\n",
    "start_time_delay_time -= pupil_phase_offset_seconds\n",
    "start_time_delay_frames -= int(np.ceil(pupil_phase_offset_seconds * FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new MATLAB session\n",
    "eng: object = matlab.engine.start_matlab()\n",
    "eng.tbUseProject(\"lightLoggerAnalysis\", nargout=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the objects we will use \n",
    "intrinsics_path: str = \"/Users/zacharykelly/Documents/MATLAB/projects/lightLoggerAnalysis/code/virtual_foveation_wip/intrinsics_calibration.mat\"\n",
    "transformation_path: str = \"/Users/zacharykelly/Documents/MATLAB/projects/lightLoggerAnalysis/code/virtual_foveation_wip/perspective_transform\"\n",
    "\n",
    "intrinsics: dict = scipy.io.loadmat(intrinsics_path)[\"camera_intrinsics_calibration\"]\n",
    "transformation: dict = scipy.io.loadmat(transformation_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5365eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformation[\"perspective_transform\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c87d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the world frames \n",
    "frames_for_video: int = len(world_frames)\n",
    "frames_transformed = []\n",
    "for world_frame_num in tqdm.trange(start_time_delay_frames, frames_for_video):    \n",
    "    print(f\"Processing frame: {world_frame_num+1}/{frames_for_video}\", flush=True)\n",
    "\n",
    "    # Retrieve the pupil frame that corresponds to this world camera frame \n",
    "    pupil_frame_num: int = world_frame_num - start_time_delay_frames\n",
    "\n",
    "    # If we have gone out of bounds for pupil, simply quit \n",
    "    if(pupil_frame_num >= len(gaze_angles)):\n",
    "        warnings.warn(f\"{pupil_frame_num} Out of bounds for pupil frame, breaking\")\n",
    "        break \n",
    "\n",
    "    # Otherwise, retrieve the gaze angles for this pupil frame \n",
    "    # and world frame \n",
    "    world_frame: np.ndarray = world_frames[world_frame_num]\n",
    "    pupil_gaze_angles: np.ndarray = gaze_angles[pupil_frame_num, :2]\n",
    "    pupil_gaze_angles[0] = -pupil_gaze_angles[0]\n",
    "    pupil_gaze_angles[1] -= 5.4\n",
    "\n",
    "    # If there is nan gaze angles, just skip this frame \n",
    "    # and leave it all 0s\n",
    "    if(np.any(np.isnan(pupil_gaze_angles))):\n",
    "        continue\n",
    "\n",
    "    # Then feed this as input into MATLAB to generate the correced frame \n",
    "    transformed_world_frame: np.ndarray = np.array(eng.coordinateTransformFinal(matlab.double(np.ascontiguousarray(world_frame).astype(np.float64)),\n",
    "                                                                                matlab.double(np.ascontiguousarray(pupil_gaze_angles.astype(np.float64))),\n",
    "                                                                                intrinsics_path, \n",
    "                                                                                transformation_path,\n",
    "                                                                                nargout=1\n",
    "                                                                               ))\n",
    "    \n",
    "    frames_transformed.append(transformed_world_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2436c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the MATLAB engine \n",
    "eng.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4ed106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Pi_util.frames_to_video(padded_frames, output_path=\"virtually_foveated_video.avi\", fps=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"inhomogenous_frames.pkl\", \"wb\") as f:\n",
    "    dill.dump(frames_transformed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cffa11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_test = None \n",
    "with open(\"inhomogenous_frames.pkl\", \"rb\") as f:\n",
    "    frames_test = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87682920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "\n",
    "frame_size_frequency_counter: collections.Counter = collections.Counter([frame.shape for frame in frames_transformed])\n",
    "max_size, count = frame_size_frequency_counter.most_common(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51490f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will iterate through the frames and pad the ones that are not max size \n",
    "padded_frames: np.ndarray = np.zeros((len(frames_transformed), *max_size[:2]), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_frame(frame, target_shape, constant_value=0):\n",
    "    \"\"\"\n",
    "    Pad a frame (H×W×C) to target_shape=(target_H, target_W[, C])\n",
    "    using constant padding (default black).\n",
    "    \"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    target_h, target_w = target_shape[:2]\n",
    "\n",
    "    # Compute padding for each side\n",
    "    pad_top = (target_h - h) // 2\n",
    "    pad_bottom = target_h - h - pad_top\n",
    "    pad_left = (target_w - w) // 2\n",
    "    pad_right = target_w - w - pad_left\n",
    "\n",
    "    # Apply padding\n",
    "    if frame.ndim == 3:\n",
    "        padded = np.pad(frame,\n",
    "                        ((pad_top, pad_bottom),\n",
    "                         (pad_left, pad_right),\n",
    "                         (0, 0)),\n",
    "                        mode='constant',\n",
    "                        constant_values=constant_value)\n",
    "    else:\n",
    "        padded = np.pad(frame,\n",
    "                        ((pad_top, pad_bottom),\n",
    "                         (pad_left, pad_right)),\n",
    "                        mode='constant',\n",
    "                        constant_values=constant_value)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fa458",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx in tqdm.tqdm(range(len(frames_transformed))):\n",
    "    try:\n",
    "        padded_frames[frame_idx] = cv2.cvtColor( frames_transformed[frame_idx], cv2.COLOR_RGB2GRAY )\n",
    "    except:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085af662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find solely the inhomogenously sized frames \n",
    "inhomogenous_frames: list = [frame for frame in frames_test if frame.shape != max_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a673dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inhomogenous_frames)/len(frames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in inhomogenous_frames:\n",
    "    plt.imshow(frame, cmap='gray')\n",
    "    plt.title(f\"Size: {frame.shape}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
