function virtuallyFoveateVideo(world_video, sensor_t_cell, gaze_angles, gaze_offsets, blnk_events, output_path, path_to_intrinsics, options)
% Virtually foveate desired frames of a video with given gaze angles
%
% Syntax:
%   virtuallyFoveateVideo(world_video, gaze_angles, offsets, output_path, path_to_intrinsics, path_to_perspective_projection, options)
%
% Description:
%   This function will virtually foveate a world camera video, given the gaze 
%   angles and offsets thereof associated with this video, as well 
%   as the original recording chunks used to generate it. 
%   It also requires the path to the intrinsics of the world camera  
%   as a camera calibration object and the path to the projection 
%   matrix that maps between screen coordinates and eye coordinates 
%   as generated by calculate_perspective_transform_w2e.m.
%   When alinging gaze angles to their associated frame, 
%   we take a nearest neighbors approach based on timestamps. 
%   We also account for a slight phase offset between the world 
%   camera and pupil camera. One may edit this, as well as the 
%   FPS of the pupil camera and the range of frames to virtually foveate. 
%   Gaze angles that are NaN are replaced with an empty frame. 
%      
% Inputs:
%   world_video                    - String. Path to the playable .avi world video
%
%   gaze_angles                    - Matrix Double. Matrix of gaze angles in px [x, y]
%                                    for each frame of the pupil camera
%   
%   gaze_offsets                   - 1x2 or 2x1 Double. Constant offset to be applied 
%                                    to the gaze angles, varies by participant
%
%   output_path                    - String. Path to the playable output .avi video
%
%
%   path_to_intrinscis             - String. Path to the camera calibration intrinsics
%                                    object for the world camera. 
%   
%   path_to_perspective_projection - String. Path to the projection object 
%                                    that maps between screen and eye coordinates. 
%                                    Generated with calculate_perspective_transform_w2e.m
% % Optional key/value pairs:
%   
%  'frames_to_process'             - 2x1 or 1x2 Double array. The range of frames 
%                                    to virtually foveate, inclusive and 1 indexed. 
%   
%  'pupil_fps'                     - Double. The FPS of the pupil camera for this recording
%
%  'pupil_world_phase_offset'      - Double. The phase offset between the pupil and world cameras 
%                                    in seconds. Applied to the pupil camera. Positive implies advanced.     
%   
% Outputs:
%
%   NONE
%
% Examples:
%{

%}

    arguments
        world_video {mustBeText}
        sensor_t_cell
        gaze_angles {mustBeMatrix}
        gaze_offsets {mustBeNumeric}
        blnk_events
        output_path {mustBeText}
        path_to_intrinsics {mustBeText}
        
        options.world_fps = 120
        options.pupil_fps {mustBeNumeric} = 120
        options.nan_deg_threshold = 45
        options.frames_to_process = [1, inf]
        options.verbose = false
        options.manual_offset = [0, 0]
        options.non_contiguous_target_frames = []
        options.testing = false
        options.video_read_cache_size = 1000
    end
    % Import the Python util library 
    if(options.verbose)
        disp("Importing Python libraries")
    end 
    virutal_foveation_util = import_pyfile(getpref("lightLoggerAnalysis", "virtual_foveation_util_path"));

    % Create a video IO reader wrapper we will use to read into the original video
    if(options.verbose)
        disp("Opening video reader/writer")
    end 
    world_frame_reader = videoIOWrapper(world_video,... 
                                        "ioAction", 'read', ...
                                        "readAheadBufferSize", options.video_read_cache_size...
                                        ); 

    if(options.testing)
        output_path = "/Users/zacharykelly/Desktop/testing.avi";
    end 

    % If we passed a group of non-contiguous target frames to analyze, 
    % the full range of the video must be inf 
    if(numel(options.non_contiguous_target_frames) > 0)
        if(options.frames_to_process ~= [1, inf])
            error("When selecting a group of non-contiguous target frames, frames to process must be [1, inf]")
        end 
    end 


    world_frame_writer = videoIOWrapper(output_path, "ioAction", 'write'); 
    world_frame_writer.FrameRate = options.world_fps; 

    % Now we will retrieve the start and end time of all of the sensors 
    if(options.verbose)
        disp("Finding sensor start end times")
    end 

    % Next, we will extract the pupil t from the gaze angles 
    pupil_t = gaze_angles(:, 1);

    % Initialize a blank frame we will use to pad frames that have nan gaze angles 
    blank_frame = uint8(zeros(480, 480)); 

    % Apply the gaze offsets to the gaze angles, and adjust their coordinate system 
    if(options.verbose)
        disp("Modifying gaze angles")
    end 
    gaze_angles_original = gaze_angles(:, 1:2) - gaze_offsets; 

    % We first subtract the constant gaze offset from the gaze angles (measured once per pariticpant)
    % Then, we flip the signs to be upsidedown and left handed. Then, 
    % we apply a manual offset from the April Tag. The sign of this corresponds to the follow
    % +azi = move right, +ele = move up
    gaze_angles(:, 1:2) = ( ( gaze_angles(:, 1:2) - gaze_offsets ) .* [1, -1] ) + options.manual_offset;

    % Extract world and pupil t 
    world_t = sensor_t_cell{1};
    pupil_t = sensor_t_cell{2};

    % Choose the bounds for our virtual foveation 
    start_frame = options.frames_to_process(1); 
    end_frame = world_frame_reader.NumFrames; 
    if(options.frames_to_process(2) ~= inf)
        end_frame = options.frames_to_process(2);
    end 

    % Open the writer to start writing frames 
    open(world_frame_writer); 

    % Iterate over the world frames 
    if(options.verbose)
        disp("Beginning frame processing")
    end 

    tic; 
    for ii = start_frame:end_frame
        if(ii > world_frame_reader.NumFrames)
            warning(sprintf("Frame %d is out of bounds for video with NumFrames %d. Quitting early.", ii, world_reader.NumFrames));
            break ; 
        end

        if(options.verbose)
            fprintf("Processing frame: %d/%d\n", ii, end_frame);
        end 

        % If we are solely processing a group of non contiguous target 
        % frames, then let's check if this frame is in that group, otherwise 
        % continue 
        if(numel(options.non_contiguous_target_frames) > 0 && ~ismember(ii, options.non_contiguous_target_frames))    
            disp("SKIPPING SINCE NOT IN TARGET FRAMES")
            continue; 
        end 

        % Retrieve the world frame timestamp 
        world_timestamp = world_t(ii); 
        
        % Find the gaze angle that corresponds to this frame 
        [~, gaze_angle_idx] = min(abs(pupil_t - world_timestamp));
        gaze_angle = gaze_angles(gaze_angle_idx, 1:2); 

        % NaN the gaze angle if it's above a large threshold
        if( any(abs(gaze_angle) > options.nan_deg_threshold) )
            disp("OVER THE THRESHOLD")
            disp(gaze_angle)
            gaze_angle(:) = nan;
        end
        
        % Virtually foveat the frame 
        virtually_foveated_frame = [];

        % If the gaze angle is NaN, immediately just 
        % use the NaN frame
        if(any(isnan(gaze_angle)))
            disp('GAZE ANGLE IS NAN')
            virtually_foveated_frame = blank_frame; 
        
        % If the pupil timestamp is during a blink event 
        % we should also just use the NaN frame 
        elseif(is_blnk_event(pupil_t(gaze_angle_idx), blnk_events))
            disp("BLINK EVENT")
            virtually_foveated_frame = blank_frame;

        % Otherwise, let's read in a real frame 
        % and virtually foveate 
        else
            % Read in the frame  
            world_frame = world_frame_reader.readFrame('frameNum', ii, 'color', 'GRAY'); 
            
            % If it is a NaN frame, just use the NaN frame for writing
            if(~any(world_frame(:))) 
                virtually_foveated_frame = blank_frame; 
            else
                virtually_foveated_frame = uint8(virtuallyFoveateFrame(world_frame, gaze_angle, path_to_intrinsics));
    
            end
        end


        % Imshow the virtually foveated frame 
        if(options.testing)
            disp(gaze_angle)
            disp(size(virtually_foveated_frame  ))

            figure; 
            imshow(virtually_foveated_frame)
            axis on; 
        end 

        % Write the frame to the video 
        world_frame_writer.writeVideo(virtually_foveated_frame);  
    end     

    % Close the world video writer 
    close(world_frame_writer); 

    elapsed_seconds = toc; 
    fprintf("Elapsed Time: %f seconds\n", elapsed_seconds); 

    return; 

end 


% Local function to determine if a given pupil frame timestamp 
% is in a range of blink events 
function is_blink = is_blnk_event(timestamp, blnk_events)
    is_blink = false; 

    % Iterate over the blnk_events
    for rr = 1:size(blnk_events, 1)
        row = blnk_events(rr, :);
        event_start = row(1);
        event_end = row(2);
        
        % If the current event end time is before the current event, we can just skip 
        if(event_end < timestamp)
            continue; 
        end 

        % If the current event start time is after the timestamp 
        % we are searching for, just return early. No event after this 
        % could be a range where this timestamp lies 
        if(event_start > timestamp)
            return; 
        end 

        % If the timestamp is in this range, it is a BLNK event, so 
        % return true 
        if((event_start <= timestamp) && (event_end >= timestamp))
            is_blink = true;
            return ; 
        end 
    end 


    return ; 

end