function virtuallyFoveateVideo(world_video, gaze_angles, gaze_offsets, output_path, path_to_recording_chunks, path_to_intrinsics, path_to_perspective_projection, options)
% Virtually foveate desired frames of a video with given gaze angles
%
% Syntax:
%   virtuallyFoveateVideo(world_video, gaze_angles, offsets, output_path, path_to_recording_chunks, path_to_intrinsics, path_to_perspective_projection, options)
%
% Description:
%   This function will virtually foveate a world camera video, given the gaze 
%   angles and offsets thereof associated with this video, as well 
%   as the original recording chunks used to generate it. 
%   It also requires the path to the intrinsics of the world camera  
%   as a camera calibration object and the path to the projection 
%   matrix that maps between screen coordinates and eye coordinates 
%   as generated by calculate_perspective_transform_w2e.m.
%   When alinging gaze angles to their associated frame, 
%   we take a nearest neighbors approach based on timestamps. 
%   We also account for a slight phase offset between the world 
%   camera and pupil camera. One may edit this, as well as the 
%   FPS of the pupil camera and the range of frames to virtually foveate. 
%   Gaze angles that are NaN are replaced with an empty frame. 
%      
% Inputs:
%   world_video                    - String. Path to the playable .avi world video
%
%   gaze_angles                    - Matrix Double. Matrix of gaze angles [azi, ele]
%                                    for each frame of the pupil camera
%   
%   gaze_offsets                   - 1x2 or 2x1 Double. Constant offset to be applied 
%                                    to the gaze angles, varies by participant
%
%   output_path                    - String. Path to the playable output .avi video
%
%   path_to_recording_chunks       - String. Path to the original recording chunks 
%                                    of this video 
%
%   path_to_intrinscis             - String. Path to the camera calibration intrinsics
%                                    object for the world camera. 
%   
%   path_to_perspective_projection - String. Path to the projection object 
%                                    that maps between screen and eye coordinates. 
%                                    Generated with calculate_perspective_transform_w2e.m
% % Optional key/value pairs:
%   
%  'num_frames_to_process'         - 2x1 or 1x2 Double array. The range of frames 
%                                    to virtually foveate, inclusive and 1 indexed. 
%   
%  'pupil_fps'                     - Double. The FPS of the pupil camera for this recording
%
%  'pupil_world_phase_offset'      - Double. The phase offset between the pupil and world cameras 
%                                    in seconds. Applied to the pupil camera. Positive implies advanced.     
%   
% Outputs:
%
%   NONE
%
% Examples:
%{


    % 2001!!!!!!!

    % First, we will define a path to the playable video of the world camera we want to virtually foveate
    % and its original chunks, to get the respective timestamps of all the sensors 
    world_video = "/Volumes/T7 Shield/scriptedIndoorOutdoorVideos/FLIC_2001/walkIndoor/temporalFrequency/W.avi"; 
    path_to_recording_chunks = "/Volumes/EXTERNAL_1/FLIC_2001/walkIndoor/temporalFrequency";

    % Load in the gaze angles and the constant offset we will apply to the gaze angles
    gaze_angles = load("/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/FLIC_analysis/lightLogger/scriptedIndoorOutdoor/FLIC_2001/walkIndoor/temporalFrequency/FLIC_2001_walkIndoor_pupilData_contrast-1x5.mat").pupilData.radiusSmoothed.eyePoses.values; 
    offsets = load("/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/FLIC_analysis/lightLogger/scriptedIndoorOutdoor/FLIC_2001/gazeCalibration/temporalFrequency/FLIC_2001_gazeCal_SceneGeometryMetadata.mat").gazeOffset;

    % Define the output path where this video will write to 
    output_path = "/Users/zacharykelly/FLIC_2001_walkIndoor_virtuallyFoveatedVideo.avi"; 
    
    % Load in the camera intrinscis of the world camera 
    path_to_intrinsics = "/Users/zacharykelly/Documents/MATLAB/projects/lightLoggerAnalysis/data/intrinsics_calibration.mat"; 

    % Load in the perspective projection object used to transform sensor positions to eye coordinates 
    % NOTE: If you do not have this, please consult calculate_perspective_transform_w2e.m 
    path_to_perspective_projection = "/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/FLIC_analysis/lightLogger/scriptedIndoorOutdoor/FLIC_2001/gazeCalibration/temporalFrequency/FLIC_2001_gazeCal_perspectiveProjection.mat";
    


    % 2003 !!!!!

    % First, we will define a path to the playable video of the world camera we want to virtually foveate
    % and its original chunks, to get the respective timestamps of all the sensors 
    world_video = "/Volumes/T7 Shield/scriptedIndoorOutdoorVideos/FLIC_2003/walkIndoor/temporalFrequency/W.avi"; 
    path_to_recording_chunks = "/Volumes/EXTERNAL_1/FLIC_2003/walkIndoor/temporalFrequency";

    % Load in the gaze angles and the constant offset we will apply to the gaze angles
    gaze_angles = load("/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/FLIC_analysis/lightLogger/scriptedIndoorOutdoor/FLIC_2003/walkIndoor/temporalFrequency/FLIC_2003_walkIndoor_pupilData_contrast-1x5.mat").pupilData.radiusSmoothed.eyePoses.values; 
    offsets = load("/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/FLIC_analysis/lightLogger/scriptedIndoorOutdoor/FLIC_2003/gazeCalibration/temporalFrequency/FLIC_2003_gazeCal_SceneGeometryMetadata.mat").gazeOffset;

    % Define the output path where this video will write to 
    output_path = "/Users/zacharykelly/FLIC_2003_walkIndoor_virtuallyFoveatedVideoAprilTag.avi"; 

    % Load in the camera intrinscis of the world camera 
    path_to_intrinsics = "/Users/zacharykelly/Documents/MATLAB/projects/lightLoggerAnalysis/data/intrinsics_calibration.mat"; 

    % Load in the perspective projection object used to transform sensor positions to eye coordinates 
    % NOTE: If you do not have this, please consult calculate_perspective_transform_w2e.m 
    path_to_perspective_projection = "/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/FLIC_analysis/lightLogger/scriptedIndoorOutdoor/FLIC_2003/gazeCalibration/temporalFrequency/FLIC_2003_gazeCal_perspectiveProjection.mat";

    start_end = [ENTER YOUR START, ENTER YOUR END];

    % Virtually foveate a PORTION of the video 
    % virtuallyFoveateVideo(world_video, gaze_angles, offsets, output_path, path_to_recording_chunks, path_to_intrinsics, path_to_perspective_projection, "num_frames_to_process", start_end, "verbose", true);


%}

    arguments 
        world_video {mustBeText}; 
        gaze_angles {mustBeMatrix}; 
        gaze_offsets {mustBeNumeric}; 
        output_path {mustBeText}; 
        path_to_recording_chunks {mustBeText};
        path_to_intrinsics {mustBeText};
        path_to_perspective_projection {mustBeText}; 
        options.num_frames_to_process = [1, inf]; 
        options.pupil_fps {mustBeNumeric} = 120; 
        options.pupil_world_phase_offset {mustBeNumeric} = 0.005; 
        options.verbose = false; 
        options.manual_offset = [0, 0];  
        % walkIndoor
        % Manual offset for FLIC_2001 = [-4.75, 4.75] 
        % Manual offset for FLIC 2003 = [-6, -1.5];
        % Manual offset for FLIC 2004 = [-11.5, -12.5]
        % Manual offset for FLIC 2005 = [-7.5, -7.5]

        % lunch 
        % Manual offset for FLIC_2001 = [2.5, 5]
        % Manual offset for FLIC_2003 = [-6.25, -2.5]
        % Manual offset for FLIC_2004 = [-6, 2.5]
        % Manual offset for FLIC_2005.= [-9, -7.5]
        % Manual offset for FLIC_2006 = [-5, 10]

        % work 
        % Manual offset for FLIC_2001 = [-3, 4.5]
        % Manual offset for FLIC_2003 = [-4.5, -2]
        % Manual offset for FLIC_2004 = [-5, -12]
        % Manual offset for FLIC 2005 = [-10, -14.5]
        % Manual offset for FLIC 2006 = [8.75, 33.75]


        options.testing = false; 
        options.nan_deg_threshold = 45; 
    end     

    % Import the Python util library 
    if(options.verbose)
        disp("Importing Python libraries")
    end 
    virutal_foveation_util = import_pyfile(getpref("lightLoggerAnalysis", "virtual_foveation_util_path"));

    % Create a video IO reader wrapper we will use to read into the original video
    if(options.verbose)
        disp("Opening video reader/writer")
    end 
    world_frame_reader = videoIOWrapper(world_video, "ioAction", 'read'); 

    if(options.testing)
        output_path = "/Users/zacharykelly/Desktop/testing.avi"
    end 

    world_frame_writer = videoIOWrapper(output_path, "ioAction", 'write'); 
    world_frame_writer.FrameRate = 120; 

    % Now we will retrieve the start and end time of all of the sensors 
    if(options.verbose)
        disp("Finding sensor start end times")
    end 
    start_ends = find_sensor_start_ends(virutal_foveation_util, path_to_recording_chunks); 
    world_start_end = start_ends.("world");
    pupil_start_end = start_ends.("pupil");

    % Create the T vectors that will be used to do mapping of gaze angles to frames, given 
    % that the sensors may sometimes be off on FPS 
    world_t = linspace(world_start_end(1), world_start_end(2), world_frame_reader.NumFrames);
    pupil_t = linspace(pupil_start_end(1), pupil_start_end(2), size(gaze_angles, 1));

    if(numel(pupil_t) ~= size(gaze_angles, 1))
        error("Miscalculation of pupil timestamps");
    end 

    if(numel(world_t) ~= world_frame_reader.NumFrames)
        error("Miscalculation of world timestamps");
    end 

    % Next, add the slight offset that we measured in the calibration procedure. That is, the pupil 
    % is actually 0.005 seconds phase advanced
    pupil_t = pupil_t + options.pupil_world_phase_offset; 

    % Initialize a blank frame we will use to pad frames that have nan gaze angles 
    blank_frame = zeros(480, 480); 

    % Apply the gaze offsets to the gaze angles, and adjust their coordinate system 
    if(options.verbose)
        disp("Modifying gaze angles")
    end 
    gaze_angles_original = gaze_angles(:, 1:2) - gaze_offsets; 

    % We first subtract the constant gaze offset from the gaze angles (measured once per pariticpant)
    % Then, we flip the signs to be upsidedown and left handed. Then, 
    % we apply a manual offset from the April Tag. The sign of this corresponds to the follow
    % +azi = move right, +ele = move up
    gaze_angles(:, 1:2) = ( ( gaze_angles(:, 1:2) - gaze_offsets ) .* [-1, -1] ) + options.manual_offset;

    % Choose the bounds for our virtual foveation 
    start_frame = options.num_frames_to_process(1); 
    end_frame = world_frame_reader.NumFrames; 
    if(options.num_frames_to_process(2) ~= inf)
        end_frame = options.num_frames_to_process(2);
    end 

    % Open the writer to start writing frames 
    open(world_frame_writer); 

    % Iterate over the world frames 
    if(options.verbose)
        disp("Beginning frame processing")
    end 
    tic; 
    for ii = start_frame:end_frame
        if(ii > world_frame_reader.NumFrames)
            warning(sprintf("Frame %d is out of bounds for video with NumFrames %d. Quitting early.", ii, world_reader.NumFrames));
            break ; 
        end 

        if(options.verbose)
            fprintf("Processing frame: %d/%d\n", ii, end_frame);
        end 

        % Retrieve the world frame timestamp 
        world_timestamp = world_t(ii); 
        
        % Find the gaze angle that corresponds to this frame 
        [~, gaze_angle_idx] = min(abs(pupil_t - world_timestamp));
        gaze_angle = gaze_angles(gaze_angle_idx, 1:2); 

        if( any(abs(gaze_angle) > options.nan_deg_threshold) )
            gaze_angle(:) = nan;
        end
        
        % Load in the world frame
        world_frame = world_frame_reader.readFrame('frameNum', ii, 'grayscale', true); 

        % Virtually foveat the frame 
        virtually_foveated_frame = [];
        
        % Ensure we only send valid world frames to be virtually foveated
        if(~any(world_frame(:))) 
            virtually_foveated_frame = blank_frame; 

        % If the gaze angle is nan, just output a blank frame 
        elseif(any(isnan(gaze_angle)))
            virtually_foveated_frame = blank_frame; 

        % If we have a valid frame to virtually foveate 
        else
            virtually_foveated_frame = uint8(virtuallyFoveateFrame(world_frame, gaze_angle, path_to_intrinsics, path_to_perspective_projection)); 
        end 


        % Imshow the virtually foveated frame 
        if(options.testing)
            disp(gaze_angle)
            disp(size(virtually_foveated_frame  ))

            figure; 
            imshow(virtually_foveated_frame)
            axis on; 
        end 

        % Write the frame to the video 
        world_frame_writer.writeVideo(virtually_foveated_frame);  

    end     

    % Close the world video writer 
    close(world_frame_writer); 

    elapsed_seconds = toc; 
    fprintf("Elapsed Time: %f seconds\n", elapsed_seconds); 

    return; 

end 

% Local function to find the start and end time of all the sensors in the recording 
function start_ends = find_sensor_start_ends(virutal_foveation_util, path_to_recording_chunks)
    % Find the start ends 
    start_ends = struct(virutal_foveation_util.find_sensor_start_end_times(path_to_recording_chunks));
    field_names = fieldnames(start_ends);
    for ff = 1:numel(field_names)
        start_ends.(field_names{ff}) = double(start_ends.(field_names{ff})); 
    end 

end 