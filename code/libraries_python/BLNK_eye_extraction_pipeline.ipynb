{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49c8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import extract_eye_features\n",
    "import dill\n",
    "import scipy\n",
    "import os\n",
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "sys.path.append(\"/Users/zacharykelly/Documents/MATLAB/projects/lightLogger/raspberry_pi_firmware/utility\")\n",
    "import Pi_util\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing the videos and other files \n",
    "path_to_video_folder: str = \"/Users/zacharykelly/Desktop/geoff_videos_blnk_folder/HERO_gka/videos\"\n",
    "\n",
    "# Generate paths to all of the videos in said folder \n",
    "video_paths: list[str] = [os.path.join(path_to_video_folder, filename)\n",
    "                          for filename in os.listdir(path_to_video_folder)\n",
    "                          if '.avi' in filename\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8d50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output folder \n",
    "output_folder_path: str = \"/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/BLNK_analysis/PuffLight/modulate/HERO_gka/videos\"\n",
    "if(not os.path.exists(output_folder_path)):\n",
    "    os.mkdir(output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc18025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prediction function\n",
    "def predict_eye_features(args: tuple) -> None:\n",
    "    # Extract the arguments from the arguments tuple \n",
    "    filepath, crop_box, target_size, temp_dir_path, output_folder_path = args\n",
    "    threshold_value: int = 225\n",
    "\n",
    "    # Define the portion of the video to crop out \n",
    "    t, b, l, r = crop_box\n",
    "\n",
    "    # Extract the name of this video\n",
    "    video_name: str = os.path.splitext(os.path.basename(filepath.rstrip('/')))[0]\n",
    "\n",
    "    # Find the FPS of the video \n",
    "    video_fps: float = Pi_util.inspect_video_FPS(filepath)\n",
    "\n",
    "    # Videos are small, so we can load them entirely in from memory \n",
    "    video_as_arr: np.ndarray = Pi_util.destruct_video(filepath, is_grayscale=True)\n",
    "\n",
    "    # Crop out only the eye from the video\n",
    "    # and set the rest of the frame to black \n",
    "    video_cropped: np.ndarray = video_as_arr[:, t:b, l:r].copy()\n",
    "    white_pixels: np.ndarray = np.mean(video_cropped, axis=(0,))  > threshold_value\n",
    "    video_cropped[:, white_pixels] = 0\n",
    "\n",
    "    # Resize the video to not a small resolution \n",
    "    y_offset = (target_size[0] - video_cropped.shape[1]) // 2\n",
    "    x_offset = (target_size[1] - video_cropped.shape[2]) // 2\n",
    "\n",
    "    video_resized: np.ndarray = np.zeros((len(video_cropped), *target_size), dtype=np.uint8)\n",
    "    video_resized[:, y_offset:y_offset + video_cropped.shape[1], x_offset:x_offset + video_cropped.shape[2]] = video_cropped\n",
    "\n",
    "    # Generate a temp video from this cropped video \n",
    "    temp_video_path: str = os.path.join(temp_dir_path, f\"temp_{video_name}.avi\")\n",
    "    Pi_util.frames_to_video(video_resized, temp_video_path, video_fps)\n",
    "\n",
    "    # Extract the eye features for this video\n",
    "    eye_features: list[dict] = extract_eye_features.extract_eye_features(temp_video_path, is_grayscale=True, visualize_results=False, pupil_feature_method='pylids', safe_execution=True)\n",
    "\n",
    "    # Save the features \n",
    "    eye_features_dict: dict = {}\n",
    "    eye_features_dict[\"eye_features\"] = eye_features\n",
    "    eye_features_dict[\"metadata\"] = {'threshold_value': {'v': threshold_value, \n",
    "                                                    'desc': \"binary mask constructed from avg cropped frame. Pixels above this value=0. Done to remove light around the eye\"\n",
    "                                                   },\n",
    "                                'crop_box': {'v': crop_box, \n",
    "                                             'desc': \"box cropped out from original video to isolate the eye (t, b, l, r)\"\n",
    "                                            },\n",
    "                                'target_size': {'v': target_size,\n",
    "                                                'desc': \"target size after crop + threshold. Eye video is centered via padding to reach this size\"},\n",
    "                                'model_names': {'v': ('pytorch-pupil-v1', 'pytorch-eyelid-v1'), \n",
    "                                                'desc': \"models used for pupil/eyelid fitting\"\n",
    "                                               }\n",
    "                               }\n",
    "    \n",
    "    \n",
    "    \n",
    "    scipy.io.savemat(os.path.join(output_folder_path, f\"{video_name}_eye_features.mat\"), \n",
    "                    {\"eye_features\": eye_features_dict}\n",
    "                    )\n",
    "    \n",
    "    # Remvove the temp avi video \n",
    "    os.remove(temp_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad5de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 4/58\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_modulatedirection-Mel_phase-3.14_trial-005_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    11160\n",
      "  Duration of video [s]:  62.00\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 263/11160 [00:08<05:55, 30.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m00\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m filepath \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m))): \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mpredict_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mpredict_eye_features\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     34\u001b[0m Pi_util\u001b[38;5;241m.\u001b[39mframes_to_video(video_resized, temp_video_path, video_fps)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Extract the eye features for this video\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m eye_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_eye_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpupil_feature_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpylids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Save the features \u001b[39;00m\n\u001b[1;32m     40\u001b[0m eye_features_dict: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:473\u001b[0m, in \u001b[0;36mextract_eye_features\u001b[0;34m(video, is_grayscale, visualize_results, pupil_feature_method, safe_execution)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_eye_features\u001b[39m(video: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[1;32m    466\u001b[0m                          is_grayscale: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m                          visualize_results: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# First, extract the pupil features of the video \u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     pupil_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_pupil_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Do not visualize single features if we want all features  \u001b[39;49;00m\n\u001b[1;32m    475\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpupil_feature_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\t\t\t\t\t\t\t                            \u001b[49m\u001b[43msafe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_execution\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# Then, extract the eyelid features \u001b[39;00m\n\u001b[1;32m    481\u001b[0m     eyelid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_eyelid_features(video, \n\u001b[1;32m    482\u001b[0m                                                           is_grayscale,\u001b[38;5;66;03m# Do not visualize single features if we want all features  \u001b[39;00m\n\u001b[1;32m    483\u001b[0m                                                           \u001b[38;5;129;01mnot\u001b[39;00m visualize_results \u001b[38;5;28;01mif\u001b[39;00m visualize_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visualize_results,\n\u001b[1;32m    484\u001b[0m \t\t\t\t\t\t\t                              safe_execution\u001b[38;5;241m=\u001b[39msafe_execution\n\u001b[1;32m    485\u001b[0m                                                          )\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:230\u001b[0m, in \u001b[0;36mextract_pupil_features\u001b[0;34m(video, is_grayscale, visualize_results, method, safe_execution)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# If desired method is pylids, then its quite easy, \u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# just call the simple analysis function \u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpylids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 230\u001b[0m     pupil_features \u001b[38;5;241m=\u001b[39m \u001b[43mpylids_analyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpupil\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Otherwise, we want to use the pupil labs method,\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     pupil_features \u001b[38;5;241m=\u001b[39m pupil_labs_analyze_video(video, is_grayscale)\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:128\u001b[0m, in \u001b[0;36mpylids_analyze_video\u001b[0;34m(video, target)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(video, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing pylids method, video argument must be string path to video\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Perform pylids feature extraction\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m pylids_out: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpylids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meyelid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43msave_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Next, convert format from dict[str, list[dict]] to a list of dictionaries\u001b[39;00m\n\u001b[1;32m    135\u001b[0m pylid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:428\u001b[0m, in \u001b[0;36manalyze_video\u001b[0;34m(eye_vid, model_name, batch_sz, eye_id, estimate_pupils, estimate_eyelids, use_ransac, timestamp_file, dest_folder, npz_file, out_file, progress_bar, constraint_eyefit, save_dlc_output, save_vid, save_npz, annot_lw, annot_clr)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_config_file), model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m config.yaml file not found\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m#a wrapper function which runs DLC to estimate keypoints\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m x,y,c \u001b[38;5;241m=\u001b[39m \u001b[43mdlc_estimate_kpts\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dlc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_pupils\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# main outputs are saved as a list of dicts\u001b[39;00m\n\u001b[1;32m    431\u001b[0m dlc_dicts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:289\u001b[0m, in \u001b[0;36mdlc_estimate_kpts\u001b[0;34m(eye_vid, path_config_file, save_dlc_output, dest_folder, batch_sz, estimate_pupils, estimate_eyelids)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmpdirname:\n\u001b[0;32m--> 289\u001b[0m         \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#change in pose_config.yml to 1\u001b[39;49;00m\n\u001b[1;32m    293\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmpdirname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPYTORCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mprint\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal config files in \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tmpdirname) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/compat.py:954\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, gputouse, save_as_csv, in_random_order, destfolder, batchsize, cropping, TFGPUinference, dynamic, modelprefix, robust_nframes, allow_growth, use_shelve, auto_track, n_tracks, animal_names, calibrate, identity_only, use_openvino, engine, **torch_kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m             torch_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batchsize\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideotype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainingsetindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingsetindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_as_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_as_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_random_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_random_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_shelve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_shelve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_track\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_track\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tracks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43manimal_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manimal_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43midentity_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentity_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcropping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function is not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:545\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, save_as_csv, in_random_order, snapshot_index, detector_snapshot_index, device, destfolder, batch_size, detector_batch_size, dynamic, ctd_conditions, ctd_tracking, top_down_dynamic, modelprefix, use_shelve, robust_nframes, transform, auto_track, n_tracks, animal_names, calibrate, identity_only, overwrite, cropping, save_as_df)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m [time\u001b[38;5;241m.\u001b[39mtime()]\n\u001b[0;32m--> 545\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     runtime\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m    553\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _generate_metadata(\n\u001b[1;32m    554\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mproject_cfg,\n\u001b[1;32m    555\u001b[0m         pytorch_config\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mmodel_cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         robust_nframes\u001b[38;5;241m=\u001b[39mrobust_nframes,\n\u001b[1;32m    563\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:204\u001b[0m, in \u001b[0;36mvideo_inference\u001b[0;34m(video, pose_runner, detector_runner, cropping, shelf_writer, robust_nframes)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m--> 204\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:142\u001b[0m, in \u001b[0;36mInferenceRunner.inference\u001b[0;34m(self, images, shelf_writer)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(data)\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_full_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_results(shelf_writer)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Process the last batch even if not full\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:203\u001b[0m, in \u001b[0;36mInferenceRunner._process_full_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes prepared inputs in batches of the desired batch size.\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:246\u001b[0m, in \u001b[0;36mInferenceRunner._process_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    242\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m     mk: v[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size] \u001b[38;5;28;01mfor\u001b[39;00m mk, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    244\u001b[0m }\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# remove processed inputs from batch\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:303\u001b[0m, in \u001b[0;36mPoseInferenceRunner.predict\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mcrop(inputs)\n\u001b[1;32m    302\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 303\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/model.py:136\u001b[0m, in \u001b[0;36mPoseModel.get_predictions\u001b[0;34m(self, outputs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract method for the forward pass of the Predictor.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        A dictionary containing the predictions of each head group\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    137\u001b[0m         name: head\u001b[38;5;241m.\u001b[39mpredictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strides[name], outputs[name])\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    139\u001b[0m     }\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_features:\n\u001b[1;32m    141\u001b[0m         predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/model.py:137\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_predictions\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Abstract method for the forward pass of the Predictor.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        A dictionary containing the predictions of each head group\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 137\u001b[0m         name: \u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strides\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name, head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    139\u001b[0m     }\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_features:\n\u001b[1;32m    141\u001b[0m         predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackbone\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/predictors/single_predictor.py:93\u001b[0m, in \u001b[0;36mHeatmapPredictor.forward\u001b[0;34m(self, stride, outputs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     locrefs \u001b[38;5;241m=\u001b[39m locrefs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[1;32m     89\u001b[0m         batch_size, height, width, num_joints, \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     90\u001b[0m     )\n\u001b[1;32m     91\u001b[0m     locrefs \u001b[38;5;241m=\u001b[39m locrefs \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocref_std\n\u001b[0;32m---> 93\u001b[0m poses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pose_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheatmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocrefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_scores:\n\u001b[1;32m     96\u001b[0m     poses[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclip(poses[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/models/predictors/single_predictor.py:-1\u001b[0m, in \u001b[0;36mHeatmapPredictor.get_pose_prediction\u001b[0;34m(self, heatmap, locref, scale_factors)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate a temp output directory \n",
    "temp_dir_path: str = './temp_blnk_pipeline'\n",
    "if(not os.path.exists(temp_dir_path)):\n",
    "    os.mkdir(temp_dir_path)\n",
    "\n",
    "# Define the crop box and target size \n",
    "t, b, l, r = 140, 275, 190, 425\n",
    "crop_box: tuple = (t, b, l, r)\n",
    "target_size: tuple = (480, 640)\n",
    "\n",
    "# Define the argument list that each function \n",
    "# will get \n",
    "args_list: list[list] = [ (filepath, crop_box, target_size, temp_dir_path, output_folder_path)\n",
    "                          for filepath in video_paths \n",
    "                        ]\n",
    "\n",
    "# Process the videos \n",
    "for video_num, video_args in enumerate(args_list):\n",
    "    filepath: str = video_args[0]\n",
    "    if(not any(f\"00{trial}\" in filepath for trial in range(5, 10))): continue\n",
    "\n",
    "    print(f\"Processing: {video_num}/{len(args_list)} | filepath: {filepath}\", flush=True)\n",
    "    predict_eye_features(video_args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c258e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
