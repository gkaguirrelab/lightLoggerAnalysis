{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49c8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc8...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import extract_eye_features\n",
    "import dill\n",
    "import scipy\n",
    "import os\n",
    "import numpy as np\n",
    "import sys \n",
    "\n",
    "sys.path.append(\"/Users/zacharykelly/Documents/MATLAB/projects/lightLogger/raspberry_pi_firmware/utility\")\n",
    "import Pi_util\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder containing the videos and other files \n",
    "path_to_video_folder: str = \"/Users/zacharykelly/Desktop/geoff_videos_blnk\"\n",
    "\n",
    "# Generate paths to all of the videos in said folder \n",
    "video_paths: list[str] = [os.path.join(path_to_video_folder, filename)\n",
    "                          for filename in os.listdir(path_to_video_folder)\n",
    "                          if '.avi' in filename\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8d50a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate output folder \n",
    "output_folder_path: str = \"/Users/zacharykelly/Aguirre-Brainard Lab Dropbox/Zachary Kelly/BLNK_analysis/PuffLight/blinkResponse/HERO_gka/videos\"\n",
    "if(not os.path.exists(output_folder_path)):\n",
    "    os.mkdir(output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc18025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prediction function\n",
    "def predict_eye_features(args: tuple) -> None:\n",
    "    # Extract the arguments from the arguments tuple \n",
    "    filepath, crop_box, target_size, temp_dir_path, output_folder_path = args\n",
    "\n",
    "    # Define the portion of the video to crop out \n",
    "    t, b, l, r = crop_box\n",
    "\n",
    "    # Extract the name of this video\n",
    "    video_name: str = os.path.splitext(os.path.basename(filepath.rstrip('/')))[0]\n",
    "\n",
    "    # Find the FPS of the video \n",
    "    video_fps: float = Pi_util.inspect_video_FPS(filepath)\n",
    "\n",
    "    # Videos are small, so we can load them entirely in from memory \n",
    "    video_as_arr: np.ndarray = Pi_util.destruct_video(filepath, is_grayscale=True)\n",
    "\n",
    "    # Crop out only the eye from the video\n",
    "    # and set the rest of the frame to black \n",
    "    video_cropped: np.ndarray = video_as_arr[:, t:b, l:r].copy()\n",
    "    white_pixels: np.ndarray = video_cropped[0] > 200\n",
    "    video_cropped[:, white_pixels] = 0\n",
    "\n",
    "    # Resize the video to not a small resolution \n",
    "    y_offset = (target_size[0] - video_cropped.shape[1]) // 2\n",
    "    x_offset = (target_size[1] - video_cropped.shape[2]) // 2\n",
    "\n",
    "    video_resized: np.ndarray = np.zeros((len(video_cropped), *target_size), dtype=np.uint8)\n",
    "    video_resized[:, y_offset:y_offset + video_cropped.shape[1], x_offset:x_offset + video_cropped.shape[2]] = video_cropped\n",
    "\n",
    "    # Generate a temp video from this cropped video \n",
    "    temp_video_path: str = os.path.join(temp_dir_path, f\"temp_{video_name}.avi\")\n",
    "    Pi_util.frames_to_video(video_resized, temp_video_path, video_fps)\n",
    "\n",
    "    # Extract the eye features for this video\n",
    "    eye_features: list[dict] = extract_eye_features.extract_eye_features(temp_video_path, is_grayscale=True, visualize_results=False, pupil_feature_method='pylids', safe_execution=True)\n",
    "\n",
    "    # Save the features \n",
    "    scipy.io.savemat(os.path.join(output_folder_path, f\"{video_name}_eye_features.mat\"), \n",
    "                    {\"eye_features\": eye_features}\n",
    "                    )\n",
    "    \n",
    "    # Remvove the temp avi video \n",
    "    os.remove(temp_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cc9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many CPU cores we have \n",
    "print(mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad5de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp2mq39387/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp2mq39387/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp2mq39387/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpav7ieodb/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpav7ieodb/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpav7ieodb/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-12_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpg67uoggz/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpg67uoggz/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpg67uoggz/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp5o9uhv_h/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp5o9uhv_h/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp5o9uhv_h/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-2_trial-25_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpw_pl090o/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpw_pl090o/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpw_pl090o/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp_mbttip8/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp_mbttip8/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp_mbttip8/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-2_sequence-1_trial-03_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpi56d50vp/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpi56d50vp/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpi56d50vp/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 23.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpzbk0mtig/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpzbk0mtig/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpzbk0mtig/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-26_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpxtekxqit/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpxtekxqit/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpxtekxqit/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmppxsunpjt/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmppxsunpjt/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmppxsunpjt/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-1_sequence-1_trial-11_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/numpy/polynomial/polynomial.py:1362: RankWarning: The fit may be poorly conditioned\n",
      "  return pu._fit(polyvander, x, y, deg, rcond, full, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpqrpjyneh/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpqrpjyneh/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpqrpjyneh/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpz0xmaehd/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpz0xmaehd/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpz0xmaehd/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-14_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/numpy/polynomial/polynomial.py:1362: RankWarning: The fit may be poorly conditioned\n",
      "  return pu._fit(polyvander, x, y, deg, rcond, full, w)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:14<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpgzq6w8dy/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpgzq6w8dy/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmpgzq6w8dy/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_220.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-eyelid-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-110.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:19<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results in /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp2sx8ghkr/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5 and /var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp2sx8ghkr/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110_full.pickle\n",
      "The videos are analyzed. Now your research can truly start!\n",
      "You can create labeled videos with 'create_labeled_video'.\n",
      "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames.\n",
      "\n",
      "['/var/folders/g_/1p95771n5l1f_f8gbbftkjy80000gn/T/tmp2sx8ghkr/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.05_block-1_sequence-2_trial-23_side-RDLC_Resnet50_santini_eyelid_detectionJul182025shuffle1_snapshot_110.h5']\n",
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze temp_blnk_pipeline/temp_HERO_gka_blinkResponse_direction-LightFlux_contrast-0.00_block-2_sequence-2_trial-06_side-R.avi\n",
      "Video metadata: \n",
      "  Overall # of frames:    450\n",
      "  Duration of video [s]:  2.50\n",
      "  fps:                    180.0\n",
      "  resolution:             w=640, h=480\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 167/450 [00:05<00:09, 29.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Process the videos \u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_num, video_args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(args_list):\n\u001b[0;32m---> 19\u001b[0m     \u001b[43mpredict_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m, in \u001b[0;36mpredict_eye_features\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     33\u001b[0m Pi_util\u001b[38;5;241m.\u001b[39mframes_to_video(video_resized, temp_video_path, video_fps)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Extract the eye features for this video\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m eye_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_eye_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpupil_feature_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpylids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Save the features \u001b[39;00m\n\u001b[1;32m     39\u001b[0m scipy\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39msavemat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_eye_features.mat\u001b[39m\u001b[38;5;124m\"\u001b[39m), \n\u001b[1;32m     40\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meye_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: eye_features}\n\u001b[1;32m     41\u001b[0m                 )\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:473\u001b[0m, in \u001b[0;36mextract_eye_features\u001b[0;34m(video, is_grayscale, visualize_results, pupil_feature_method, safe_execution)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_eye_features\u001b[39m(video: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[1;32m    466\u001b[0m                          is_grayscale: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    467\u001b[0m                          visualize_results: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \n\u001b[1;32m    472\u001b[0m     \u001b[38;5;66;03m# First, extract the pupil features of the video \u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m     pupil_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_pupil_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Do not visualize single features if we want all features  \u001b[39;49;00m\n\u001b[1;32m    475\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpupil_feature_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\t\t\t\t\t\t\t\u001b[49m\u001b[43msafe_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_execution\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m                                                       \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# Then, extract the eyelid features \u001b[39;00m\n\u001b[1;32m    481\u001b[0m     eyelid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_eyelid_features(video, \n\u001b[1;32m    482\u001b[0m                                                           is_grayscale,\u001b[38;5;66;03m# Do not visualize single features if we want all features  \u001b[39;00m\n\u001b[1;32m    483\u001b[0m                                                           \u001b[38;5;129;01mnot\u001b[39;00m visualize_results \u001b[38;5;28;01mif\u001b[39;00m visualize_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visualize_results,\n\u001b[1;32m    484\u001b[0m \t\t\t\t\t\t\t  safe_execution\u001b[38;5;241m=\u001b[39msafe_execution\n\u001b[1;32m    485\u001b[0m                                                          )\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:230\u001b[0m, in \u001b[0;36mextract_pupil_features\u001b[0;34m(video, is_grayscale, visualize_results, method, safe_execution)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# If desired method is pylids, then its quite easy, \u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# just call the simple analysis function \u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpylids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 230\u001b[0m     pupil_features \u001b[38;5;241m=\u001b[39m \u001b[43mpylids_analyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpupil\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Otherwise, we want to use the pupil labs method,\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     pupil_features \u001b[38;5;241m=\u001b[39m pupil_labs_analyze_video(video, is_grayscale)\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:128\u001b[0m, in \u001b[0;36mpylids_analyze_video\u001b[0;34m(video, target)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(video, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing pylids method, video argument must be string path to video\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Perform pylids feature extraction\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m pylids_out: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpylids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meyelid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43msave_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Next, convert format from dict[str, list[dict]] to a list of dictionaries\u001b[39;00m\n\u001b[1;32m    135\u001b[0m pylid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:428\u001b[0m, in \u001b[0;36manalyze_video\u001b[0;34m(eye_vid, model_name, batch_sz, eye_id, estimate_pupils, estimate_eyelids, use_ransac, timestamp_file, dest_folder, npz_file, out_file, progress_bar, constraint_eyefit, save_dlc_output, save_vid, save_npz, annot_lw, annot_clr)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_config_file), model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m config.yaml file not found\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m#a wrapper function which runs DLC to estimate keypoints\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m x,y,c \u001b[38;5;241m=\u001b[39m \u001b[43mdlc_estimate_kpts\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dlc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_pupils\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# main outputs are saved as a list of dicts\u001b[39;00m\n\u001b[1;32m    431\u001b[0m dlc_dicts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:289\u001b[0m, in \u001b[0;36mdlc_estimate_kpts\u001b[0;34m(eye_vid, path_config_file, save_dlc_output, dest_folder, batch_sz, estimate_pupils, estimate_eyelids)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmpdirname:\n\u001b[0;32m--> 289\u001b[0m         \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#change in pose_config.yml to 1\u001b[39;49;00m\n\u001b[1;32m    293\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmpdirname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPYTORCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mprint\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal config files in \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tmpdirname) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/compat.py:954\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, gputouse, save_as_csv, in_random_order, destfolder, batchsize, cropping, TFGPUinference, dynamic, modelprefix, robust_nframes, allow_growth, use_shelve, auto_track, n_tracks, animal_names, calibrate, identity_only, use_openvino, engine, **torch_kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m             torch_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batchsize\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideotype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainingsetindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingsetindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_as_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_as_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_random_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_random_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_shelve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_shelve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_track\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_track\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tracks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43manimal_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manimal_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43midentity_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentity_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcropping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function is not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:545\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, save_as_csv, in_random_order, snapshot_index, detector_snapshot_index, device, destfolder, batch_size, detector_batch_size, dynamic, ctd_conditions, ctd_tracking, top_down_dynamic, modelprefix, use_shelve, robust_nframes, transform, auto_track, n_tracks, animal_names, calibrate, identity_only, overwrite, cropping, save_as_df)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m [time\u001b[38;5;241m.\u001b[39mtime()]\n\u001b[0;32m--> 545\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     runtime\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m    553\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _generate_metadata(\n\u001b[1;32m    554\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mproject_cfg,\n\u001b[1;32m    555\u001b[0m         pytorch_config\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mmodel_cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         robust_nframes\u001b[38;5;241m=\u001b[39mrobust_nframes,\n\u001b[1;32m    563\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:204\u001b[0m, in \u001b[0;36mvideo_inference\u001b[0;34m(video, pose_runner, detector_runner, cropping, shelf_writer, robust_nframes)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m--> 204\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:142\u001b[0m, in \u001b[0;36mInferenceRunner.inference\u001b[0;34m(self, images, shelf_writer)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(data)\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_full_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_results(shelf_writer)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Process the last batch even if not full\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:203\u001b[0m, in \u001b[0;36mInferenceRunner._process_full_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes prepared inputs in batches of the desired batch size.\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:246\u001b[0m, in \u001b[0;36mInferenceRunner._process_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m    242\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    243\u001b[0m     mk: v[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size] \u001b[38;5;28;01mfor\u001b[39;00m mk, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    244\u001b[0m }\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# remove processed inputs from batch\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:310\u001b[0m, in \u001b[0;36mPoseInferenceRunner.predict\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     {\n\u001b[1;32m    312\u001b[0m         head: {\n\u001b[1;32m    313\u001b[0m             pred_name: pred[b]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m pred_name, pred \u001b[38;5;129;01min\u001b[39;00m head_outputs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    315\u001b[0m         }\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m head, head_outputs \u001b[38;5;129;01min\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    317\u001b[0m     }\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)\n\u001b[1;32m    319\u001b[0m ]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:311\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    310\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 311\u001b[0m     {\n\u001b[1;32m    312\u001b[0m         head: {\n\u001b[1;32m    313\u001b[0m             pred_name: pred[b]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m pred_name, pred \u001b[38;5;129;01min\u001b[39;00m head_outputs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    315\u001b[0m         }\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m head, head_outputs \u001b[38;5;129;01min\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    317\u001b[0m     }\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)\n\u001b[1;32m    319\u001b[0m ]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:312\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    310\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     {\n\u001b[0;32m--> 312\u001b[0m         head: {\n\u001b[1;32m    313\u001b[0m             pred_name: pred[b]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m pred_name, pred \u001b[38;5;129;01min\u001b[39;00m head_outputs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    315\u001b[0m         }\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m head, head_outputs \u001b[38;5;129;01min\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    317\u001b[0m     }\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)\n\u001b[1;32m    319\u001b[0m ]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:313\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m    307\u001b[0m         raw_predictions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodypart\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    310\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     {\n\u001b[1;32m    312\u001b[0m         head: {\n\u001b[0;32m--> 313\u001b[0m             pred_name: \u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    314\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m pred_name, pred \u001b[38;5;129;01min\u001b[39;00m head_outputs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    315\u001b[0m         }\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m head, head_outputs \u001b[38;5;129;01min\u001b[39;00m raw_predictions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    317\u001b[0m     }\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)\n\u001b[1;32m    319\u001b[0m ]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate a temp output directory \n",
    "temp_dir_path: str = './temp_blnk_pipeline'\n",
    "if(not os.path.exists(temp_dir_path)):\n",
    "    os.mkdir(temp_dir_path)\n",
    "\n",
    "# Define the crop box and target size \n",
    "t, b, l, r = 140, 275, 190, 425\n",
    "crop_box: tuple = (t, b, l, r)\n",
    "target_size: tuple = (480, 640)\n",
    "\n",
    "# Define the argument list that each function \n",
    "# will get \n",
    "args_list: list[list] = [ (filepath, crop_box, target_size, temp_dir_path, output_folder_path)\n",
    "                          for filepath in video_paths \n",
    "                        ]\n",
    "\n",
    "# Process the videos \n",
    "for video_num, video_args in enumerate(args_list):\n",
    "    print(f\"Processing: {video_num}/{len(args_list)}\", flush=True)\n",
    "    predict_eye_features(video_args)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c258e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
