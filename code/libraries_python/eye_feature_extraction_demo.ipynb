{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49c2727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLC 3.0.0rc8...\n",
      "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries \n",
    "import extract_eye_features\n",
    "import dill\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "068be452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supply a path to a video\n",
    "path_to_video: str = \"/Volumes/T7 Shield/eye0.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fad1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights already exist!\n",
      "Analyzing videos with /Users/zacharykelly/Library/Application Support/pylids/pytorch-pupil-v1/dlc-models-pytorch/iteration-0/santini_eyelid_detectionJul182025-trainset99shuffle1/train/snapshot-best-220.pt\n",
      "Starting to analyze /Volumes/T7 Shield/eye0.mp4\n",
      "Video metadata: \n",
      "  Overall # of frames:    4580\n",
      "  Duration of video [s]:  38.17\n",
      "  fps:                    119.98\n",
      "  resolution:             w=400, h=400\n",
      "\n",
      "Running pose prediction with batch size 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 3408/4580 [01:20<00:27, 42.72it/s]Process Process-1:\n",
      " 74%|███████▍  | 3412/4580 [01:20<00:27, 42.46it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/zacharykelly/Documents/MATLAB/projects/lightLogger/raspberry_pi_firmware/utility/Pi_util.py\", line 264, in destruct_video\n",
      "    q.put(frame)\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/queues.py\", line 89, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/threading.py\", line 1096, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/anaconda3/envs/pylids/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Attempt to extract eye features from this video \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m eye_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mextract_eye_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_eye_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_to_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grayscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpupil_feature_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpylids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#pupil_features: list[dict] = extract_eye_features.extract_pupil_features(path_to_video, is_grayscale=True, visualize_results=True, method='pupil-labs')\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:463\u001b[0m, in \u001b[0;36mextract_eye_features\u001b[0;34m(video, is_grayscale, visualize_results, pupil_feature_method)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_eye_features\u001b[39m(video: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray, \n\u001b[1;32m    457\u001b[0m                          is_grayscale: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    458\u001b[0m                          visualize_results: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# First, extract the pupil features of the video \u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m     pupil_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_pupil_features(video, \n\u001b[1;32m    464\u001b[0m                                                         is_grayscale, \u001b[38;5;66;03m# Do not visualize single features if we want all features  \u001b[39;00m\n\u001b[1;32m    465\u001b[0m                                                         \u001b[38;5;129;01mnot\u001b[39;00m visualize_results \u001b[38;5;28;01mif\u001b[39;00m visualize_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visualize_results, \n\u001b[1;32m    466\u001b[0m                                                         method\u001b[38;5;241m=\u001b[39mpupil_feature_method\n\u001b[1;32m    467\u001b[0m                                                        )\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# Then, extract the eyelid features \u001b[39;00m\n\u001b[1;32m    470\u001b[0m     eyelid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_eyelid_features(video, \n\u001b[1;32m    471\u001b[0m                                                           is_grayscale,\u001b[38;5;66;03m# Do not visualize single features if we want all features  \u001b[39;00m\n\u001b[1;32m    472\u001b[0m                                                           \u001b[38;5;129;01mnot\u001b[39;00m visualize_results \u001b[38;5;28;01mif\u001b[39;00m visualize_results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visualize_results,\n\u001b[1;32m    473\u001b[0m                                                          )\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:228\u001b[0m, in \u001b[0;36mextract_pupil_features\u001b[0;34m(video, is_grayscale, visualize_results, method)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# If desired method is pylids, then its quite easy, \u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# just call the simple analysis function \u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpylids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 228\u001b[0m     pupil_features \u001b[38;5;241m=\u001b[39m pylids_analyze_video(video, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpupil\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;66;03m# Otherwise, we want to use the pupil labs method,\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     pupil_features \u001b[38;5;241m=\u001b[39m pupil_labs_analyze_video(video, is_grayscale)\n",
      "File \u001b[0;32m~/Documents/MATLAB/projects/lightLoggerAnalysis/code/libraries_python/extract_eye_features.py:127\u001b[0m, in \u001b[0;36mpylids_analyze_video\u001b[0;34m(video, target)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(video, \u001b[38;5;28mstr\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing pylids method, video argument must be string path to video\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Perform pylids feature extraction\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m pylids_out: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpylids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meyelid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpytorch-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43msave_vid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m \u001b[43m                                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Next, convert format from dict[str, list[dict]] to a list of dictionaries\u001b[39;00m\n\u001b[1;32m    134\u001b[0m pylid_features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:428\u001b[0m, in \u001b[0;36manalyze_video\u001b[0;34m(eye_vid, model_name, batch_sz, eye_id, estimate_pupils, estimate_eyelids, use_ransac, timestamp_file, dest_folder, npz_file, out_file, progress_bar, constraint_eyefit, save_dlc_output, save_vid, save_npz, annot_lw, annot_clr)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path_config_file), model_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m config.yaml file not found\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m#a wrapper function which runs DLC to estimate keypoints\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m x,y,c \u001b[38;5;241m=\u001b[39m \u001b[43mdlc_estimate_kpts\u001b[49m\u001b[43m(\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dlc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_pupils\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimate_eyelids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# main outputs are saved as a list of dicts\u001b[39;00m\n\u001b[1;32m    431\u001b[0m dlc_dicts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/pylids/pylids.py:289\u001b[0m, in \u001b[0;36mdlc_estimate_kpts\u001b[0;34m(eye_vid, path_config_file, save_dlc_output, dest_folder, batch_sz, estimate_pupils, estimate_eyelids)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmpdirname:\n\u001b[0;32m--> 289\u001b[0m         \u001b[43mdeeplabcut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_config_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m[\u001b[49m\u001b[43meye_vid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_sz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#change in pose_config.yml to 1\u001b[39;49;00m\n\u001b[1;32m    293\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmpdirname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPYTORCH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m         \u001b[38;5;28mprint\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmpdirname,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal config files in \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(tmpdirname) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not equal to 1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/compat.py:954\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, gputouse, save_as_csv, in_random_order, destfolder, batchsize, cropping, TFGPUinference, dynamic, modelprefix, robust_nframes, allow_growth, use_shelve, auto_track, n_tracks, animal_names, calibrate, identity_only, use_openvino, engine, **torch_kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m             torch_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batchsize\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43manalyze_videos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideotype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideotype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainingsetindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingsetindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_as_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_as_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_random_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_random_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdestfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodelprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_shelve\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_shelve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauto_track\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_track\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tracks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tracks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43manimal_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manimal_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcalibrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcalibrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43midentity_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midentity_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcropping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcropping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtorch_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function is not implemented for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mengine\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:545\u001b[0m, in \u001b[0;36manalyze_videos\u001b[0;34m(config, videos, videotype, shuffle, trainingsetindex, save_as_csv, in_random_order, snapshot_index, detector_snapshot_index, device, destfolder, batch_size, detector_batch_size, dynamic, ctd_conditions, ctd_tracking, top_down_dynamic, modelprefix, use_shelve, robust_nframes, transform, auto_track, n_tracks, animal_names, calibrate, identity_only, overwrite, cropping, save_as_df)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    544\u001b[0m     runtime \u001b[38;5;241m=\u001b[39m [time\u001b[38;5;241m.\u001b[39mtime()]\n\u001b[0;32m--> 545\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_iterator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpose_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_runner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrobust_nframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrobust_nframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     runtime\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime())\n\u001b[1;32m    553\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m _generate_metadata(\n\u001b[1;32m    554\u001b[0m         cfg\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mproject_cfg,\n\u001b[1;32m    555\u001b[0m         pytorch_config\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mmodel_cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m         robust_nframes\u001b[38;5;241m=\u001b[39mrobust_nframes,\n\u001b[1;32m    563\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:204\u001b[0m, in \u001b[0;36mvideo_inference\u001b[0;34m(video, pose_runner, detector_runner, cropping, shelf_writer, robust_nframes)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m--> 204\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpose_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelf_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshelf_writer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shelf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     shelf_writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/runners/inference.py:140\u001b[0m, in \u001b[0;36mInferenceRunner.inference\u001b[0;34m(self, images, shelf_writer)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run model inference on the given dataset\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mTODO: Add an option to also return head outputs (such as heatmaps)? Can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    ]\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(data)\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_full_batches()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pylids/lib/python3.10/site-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py:97\u001b[0m, in \u001b[0;36mVideoIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Otherwise ValueError: At least one stride in the given numpy array is negative,\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# and tensors with negative strides are not currently supported. (You can probably\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# work around this by making a copy of your array  with array.copy().)\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Attempt to extract eye features from this video \n",
    "#eye_features: list[dict] = extract_eye_features.extract_eye_features(path_to_video, is_grayscale=True, visualize_results=True, pupil_feature_method='pylids')\n",
    "pupil_features: list[dict] = extract_eye_features.extract_pupil_features(path_to_video, is_grayscale=True, visualize_results=True, method='pupil-labs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958284fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optionally) Output the results to a mat file \n",
    "scipy.io.savemat(\"eye_features.mat\", {\"eye_features\": eye_features})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pylids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
